{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inclass: Unsupervised Learning**\n",
    "- Durasi: 7 hours\n",
    "- _Last Updated_: Desember 2023\n",
    "\n",
    "___\n",
    "\n",
    "- Disusun dan dikurasi oleh tim produk dan instruktur [Algoritma Data Science School](https://algorit.ma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Machine learning berfokus pada prediksi berdasarkan properti/fitur yang dipelajari dari data training. Beberapa tipe machine learning yaitu:\n",
    "\n",
    "\n",
    "**Supervised Learning**: \n",
    "\n",
    "* memiliki target variable. \n",
    "* untuk pembuatan model prediksi $(y \\sim x)$\n",
    "* ada ground truth (label aktual) sehingga ada evaluasi model\n",
    "\n",
    "**Unsupervised Learning**: \n",
    "\n",
    "* tidak memiliki target variable. \n",
    "* untuk mencari pola dalam data sehingga menghasilkan informasi yang berguna/dapat diolah lebih lanjut. umumnya dipakai untuk tahap explanatory data analysis (EDA)/data pre-processing.\n",
    "* tidak ada ground truth sehingga sulit mengevaluasi model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Tujuan dimensionality reduction adalah untuk **mereduksi banyaknya variabel (dimensi/fitur)** pada data dengan tetap **mempertahankan informasi sebanyak mungkin**. Dimensionality reduction dapat mengatasi masalah high-dimensional data. Kesulitan yang dihadapi pada high-dimensional data:\n",
    "\n",
    "- Memerlukan waktu dan komputasi yang besar dalam melakukan pemodelan\n",
    "- Melakukan visualisasi lebih dari tiga dimensi\n",
    "- Menyulitkan pengolahan data (feature selection)\n",
    "\n",
    "Note:\n",
    "\n",
    "* **Dimensi**: kolom, semakin banyak kolom maka dimensi semakin tinggi.\n",
    "* **Informasi**: [variance](#Glossary), semakin tinggi variance maka informasinya semakin banyak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah data gaji perusahaan A dan B dalam **satuan juta rupiah**. \n",
    "\n",
    "Pertanyaan: Tanpa menghitung nilai [variance](#Glossary), perusahaan mana yang memiliki gaji lebih bervariasi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba bandingkan variansi kedua data ini:\n",
    "A = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "B = [4, 5, 5, 6, 6, 4, 6, 5, 4, 4]\n",
    "\n",
    "print(np.var(A))\n",
    "print(np.var(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è Note:</b> variansi  bergantung pada skala variable \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada pula data gaji perusahaan C dalam **satuan dollar**. Untuk mempermudah, asumsi 1 dollar = 10000 rupiah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "np.var(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apakah bisa dibilang gaji di perusahaan C lebih bervariasi daripada A?\n",
    "\n",
    "> Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation Example: Image Compression\n",
    "\n",
    "Pada data gambar, setiap kotak pixel akan menjadi 1 kolom. Foto berukuran 40x40 pixel memiliki 1600 kolom (dimensi). Sekarang mari renungkan, berapa spesifikasi kamera handphone anda? Berapa besar dimensi data yang dihasilkan kamera Anda?\n",
    "\n",
    "Image compression adalah salah satu contoh nyata dimensionality reduction menggunakan data gambar yang  dan tetap menghasilkan gambar yang serupa (informasi inti tidak hilang), sehingga data gambar lebih mudah diproses. Salah satu algoritma yang dapat digunakan untuk dimensionality reduction adalah **Principal Component Analysis (PCA)**.\n",
    "\n",
    "\n",
    "<img src=\"assets/cat_pca.png\" width=\"700\">\n",
    "    \n",
    "<a href=\"https://www.tandfonline.com/doi/pdf/10.1080/09500340.2016.1270881\" style=\"margin:auto; display:block;\" class=\"button large hpbottom\">alternatives on lenna image</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Knowledge Check:**\n",
    "\n",
    "Dalam suatu gambar apa yang dimaksud dengan dimensi dan informasi?\n",
    "\n",
    "- dimensi : ____\n",
    "- informasi: ____\n",
    "\n",
    "\n",
    "Apakah nilai dari variansi dipengaruhi oleh skala dari nilai itu sendiri? jelaskan!\n",
    "\n",
    "> Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Principle Component Analysis\n",
    "\n",
    "### Konsep\n",
    "\n",
    "Ide dasar dari PCA adalah untuk membuat sumbu (axis) baru yang dapat menangkap informasi sebesar mungkin. Sumbu baru ini adalah yang dinamakan sebagai Principal Component (PC). Untuk melakukan dimensionality reduction, kita akan memilih beberapa PC untuk dapat merangkum informasi yang dibutuhkan\n",
    "\n",
    "<img src=\"assets/ul10.JPG\" width=\"700\">\n",
    "\n",
    "**Figure A (Sebelum PCA):**\n",
    "\n",
    "- Sumbu/dimensi: X1 dan X2\n",
    "- Variance data dijelaskan oleh X1 dan X2\n",
    "- Dibuatlah sumbu baru untuk menangkap informasi X1 dan X2, yang dinamakan PC1 dan PC2\n",
    "\n",
    "**Figure B (Setelah PCA):**\n",
    "\n",
    "- Sumbu baru: PC1 dan PC2\n",
    "- PC1 menangkap variance lebih banyak daripada PC2\n",
    "- Misalkan PC1 menangkap 90% variance, dan sisanya ditangkap oleh PC2 yaitu 10%\n",
    "\n",
    "üí° **Notes**:\n",
    "\n",
    "- Membuat sumbu baru yang disebut dengan PC yang bertujuan untuk merangkum sebanyak mungkin informasi data\n",
    "- Banyaknya jumlah PC sama dengan jumlah dimensi dari data\n",
    "- PC1 pasti menangkap variance paling besar dibandingkan dengan PC 2, dan seterusnya\n",
    "- Antara PC1 dan PC2 saling tegak lurus, artinya tidak saling berkorelasi\n",
    "- Metode PCA akan cocok untuk data numerik yang saling berkorelasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚úÖ Knowledge Check:**\n",
    "\n",
    "<img src=\"assets/knowledge check.png\" width=\"500\">\n",
    "\n",
    "1.  Dari Gambar diatas mana data yang cocok dilakukan PCA?\n",
    "\n",
    "-   [ ] Sale Price of Vehicles\n",
    "-   [ ] Blind Tasting\n",
    "-   [ ] Logistic Machinery\n",
    "\n",
    "2.  Bila terdapat 3 PC, PC ke-berapa yang merangkum variansi (informasi) paling besar?\n",
    "\n",
    "-   [ ] PC1\n",
    "-   [ ] PC2\n",
    "-   [ ] PC3\n",
    "\n",
    "3.  Dalam PCA jumlah PC yang dihasilkan sebanyak....\n",
    "\n",
    "-   [ ] Jumlah variabel yang digunakan\n",
    "-   [ ] Setengah dari jumlah variabel yang digunakan\n",
    "-   [ ] Ditentukan oleh user \n",
    "\n",
    "4.  PC1 dibentuk oleh variabel pertama dan PC4 dibentuk oleh variabel ke empat\n",
    "\n",
    "-   [ ] Salah\n",
    "-   [ ] Benar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Behind PCA [optional]\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>&#128250; Rekomendasi Video:</b> <a href=\"https://www.youtube.com/watch?v=PFDu9oVAE-g\" class=\"button large hpbottom\">3Blue1Brown: Eigenvectors and eigenvalues</a>\n",
    "</div>\n",
    "\n",
    "Untuk membentuk PC dibutuhkan **eigen values** & **eigen vector**. Secara manual, eigen values dan eigen vector didapatkan dari operasi matrix.\n",
    "\n",
    "Teori matrix:\n",
    "\n",
    "* skalar: nilai yang memiliki magnitude/besaran\n",
    "* vektor: nilai yang memiliki besaran dan arah (umum digambarkan dalam suatu koordinat)\n",
    "* matrix: kumpulan nilai/bentukan data dalam baris dan kolom\n",
    "\n",
    "\n",
    "**Eigen- dari suatu Matrix**\n",
    "\n",
    "Untuk setiap matrix $A$, terdapat **vektor spesial (eigen vector)** yang jika dikalikan dengan matrixnya, hasilnya akan sama dengan vektor tersebut dikalikan suatu **skalar (eigen value)**. Sehingga didapatkan rumus:\n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "\n",
    "dengan $x$ adalah eigen vector dan $\\lambda$ adalah eigen value dari matrix $A$.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "Pada perhitungan matrix di bawah, salah satu eigen vector dari matrix \n",
    "$\\begin{bmatrix}\n",
    "2 & 3\\\\ \n",
    "2 & 1\n",
    "\\end{bmatrix}$\n",
    "adalah \n",
    "$\\begin{bmatrix}\n",
    "3\\\\ \n",
    "2\n",
    "\\end{bmatrix}$\n",
    "dengan eigen value sebesar 4.\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "2 & 3\\\\ \n",
    "2 & 1 \n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cc} \n",
    "3\\\\ \n",
    "2\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{cc} \n",
    "12\\\\ \n",
    "8\n",
    "\\end{array}\\right)\n",
    "=4\n",
    "\\left(\\begin{array}{cc} \n",
    "3\\\\ \n",
    "2\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teori eigen dipakai untuk menentukan PC dan nilai-nilai pada PC.\n",
    "\n",
    "**Penerapan Eigen dalam PCA:**\n",
    "\n",
    "**Matrix [covariance](#Glossary)** adalah matrix yang dapat merangkum informasi (variance) dari data. Kita menggunakan matrix covariance untuk mendapatkan eigen vector dan eigen value dari matrix tersebut, dengan:\n",
    "\n",
    "* **eigen vector**: arah sumbu tiap PC, yang menjadi formula untuk mentransformasi data awal ke PC baru. \n",
    "* **eigen value**: variansi yang ditangkap oleh setiap PC.\n",
    "* tiap PC memiliki 1 eigen value & 1 eigen vector.\n",
    "* alur: matrix covariance $\\rightarrow$ eigen value $\\rightarrow$ eigen vector $\\rightarrow$ nilai di tiap PC\n",
    "\n",
    "Eigen vector akan menjadi formula untuk kalkulasi nilai di setiap PC. Contohnya, untuk data yang terdiri dari 2 variabel, bila diketahui eigen vector dari PC1 adalah:\n",
    "\n",
    "$$x_{PC1}= \\left[\\begin{array}{cc}a_1\\\\a_2\\end{array}\\right]$$\n",
    "\n",
    "Maka formula untuk menghitung nilai pada PC1 (untuk tiap barisnya) adalah:\n",
    "\n",
    "$$PC1= a_1X_1 + a_2X_2$$\n",
    "\n",
    "Keterangan:\n",
    "\n",
    "* $x_{PC1}$ : eigen vector PC1 dari matrix covariance\n",
    "* $a_1$, $a_2$ : konstanta dari eigen vector\n",
    "* $PC1$ : nilai di PC1\n",
    "* $X_1$, $X_2$ : nilai variabel X1 dan X2 di data awal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contoh menghitung eigen value dan eigen vector dari sebuah data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat data dummy\n",
    "dummy = pd.DataFrame(np.random.rand(4, 2), #generate random value dengan 4 baris dan 2 kolom\n",
    "                 columns=list('XY')) #nama tiap kolom\n",
    "dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencari nilai [covariance](#Glossary) pada dataframe dummy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_cov = dummy.cov()\n",
    "matrix_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencari nilai dan vector eigen dengan fungsi [eig](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) dari library [numpy](https://numpy.org/doc/stable/index.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals,eig_vecs = eig(matrix_cov.T) \n",
    "print('E-value: \\n', eig_vals) #\\n untuk newline (enter ke bawah)\n",
    "print('E-vector: \\n', eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: hasil fungsi eig() tidak berurutan berdasarkan nilainya. Eigenvalues dari PC1 adalah nilai terbesar, dilanjutkan PC2 dengan nilai kedua terbesar dan seterusnya.    \n",
    "\n",
    "* `E-value:`: Eigen value untuk tiap PC, besar variansi yang dapat ditangkap oleh tiap PC. Eigen value tertinggi adalah milik PC1, kedua tertinggi milik PC2, dan seterusnya. \n",
    "\n",
    "* `E-vector`: Eigen vector untuk tiap PC. Kolom `eig_vecs[:,i]` adalah vektor eigen yang sesuai dengan nilai eigen `eig_vals[i]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Workflow\n",
    "\n",
    "#### Business Question: Dimensionality Reduction for Fraud Bank Account dataset\n",
    "\n",
    "Kita akan kembali menggunakan data `fraud_dataset.csv` yang sudah digunakan pada pembelajaran sebelumnya. Perbedaannya adalah kita akan menggunakan keseluruhan kolom pada data ini dan hanya akan membuang kolom yang kemaren kita jadikan sebagai target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = pd.read_csv('data_input/fraud_dataset.csv')\n",
    "fraud.drop(columns=['fraud_bool'], inplace=True)\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Dataset**\n",
    "\n",
    "Berikut adalah penjelasan setiap kolom yang terdapat pada _dataset_:\n",
    "\n",
    "- `income` (numeric): _Annual income of the applicant (in decile form). Ranges between [0.1, 0.9]._\n",
    "- `name_email_similarity` (numeric): _Metric of similarity between email and applicant‚Äôs name. Higher values represent higher similarity. Ranges between [0, 1]._\n",
    "- `current_address_months_count` (numeric): _Months in currently registered address of the applicant. Ranges between [‚àí1, 429] months (-1 is a missing value)._\n",
    "- `customer_age` (numeric): _Applicant‚Äôs age in years, rounded to the decade. Ranges between [10, 90] years._\n",
    "- `days_since_request` (numeric): _Number of days passed since application was done. Ranges between [0, 79] days._\n",
    "- `intended_balcon_amount` (numeric): _Initial transferred amount for application. Ranges between [‚àí16, 114] (negatives are missing values)._\n",
    "- `payment_type` (categorical): _Credit payment plan type. 5 possible (annonymized) values._\n",
    "- `zip_count_4w` (numeric): _Number of applications within same zip code in last 4 weeks. Ranges between [1, 6830]._\n",
    "- `velocity_6h` (numeric): _Velocity of total applications made in last 6 hours i.e., average number of applications per hour in the last 6 hours. Ranges between [‚àí175, 16818]._\n",
    "- `velocity_24h` (numeric): _Velocity of total applications made in last 24 hours i.e., average number of applications per hour in the last 24 hours. Ranges between [1297, 9586]_\n",
    "- `velocity_4w` (numeric): _Velocity of total applications made in last 4 weeks, i.e., average number of applications per hour in the last 4 weeks. Ranges between [2825, 7020]._\n",
    "- `bank_branch_count_8w` (numeric): _Number of total applications in the selected bank branch in last 8 weeks. Ranges between [0, 2404]._\n",
    "- `date_of_birth_distinct_emails_4w` (numeric): _Number of emails for applicants with same date of birth in last 4 weeks. Ranges between [0, 39]._\n",
    "- `employment_status` (categorical): _Employment status of the applicant. 7 possible (annonymized) values._\n",
    "- `credit_risk_score` (numeric): _Internal score of application risk. Ranges between [‚àí191, 389]._\n",
    "- `email_is_free` (binary): _Domain of application email (either free or paid)._\n",
    "- `housing_status` (categorical): _Current residential status for applicant. 7 possible (annonymized) values._\n",
    "- `phone_home_valid` (binary): _Validity of provided home phone._\n",
    "- `phone_mobile_valid` (binary): _Validity of provided mobile phone._\n",
    "- `has_other_cards` (binary): _If applicant has other cards from the same banking company. _\n",
    "- `proposed_credit_limit` (numeric): _Applicant‚Äôs proposed credit limit. Ranges between [200, 2000]._\n",
    "- `foreign_request` (binary): _If origin country of request is different from bank‚Äôs country._\n",
    "- `source` (categorical): _Online source of application. Either browser (INTERNET) or app (TELEAPP)._\n",
    "- `session_length_in_minutes` (numeric): _Length of user session in banking website in minutes. Ranges between [‚àí1, 107] minutes (-1 is a missing value)._\n",
    "- `device_os` (categorical): _Operative system of device that made request. Possible values are: Windows, macOS, Linux, X11, or other._\n",
    "- `keep_alive_session` (binary): _User option on session logout._\n",
    "- `device_distinct_emails` (numeric): _Number of distinct emails in banking website from the used device in last 8 weeks. Ranges between [‚àí1, 2] emails (-1 is a missing value)._\n",
    "- `month` (numeric): _Month where the application was made. Ranges between [0, 7]._\n",
    "- `fraud_bool` (binary): _If the application is fraudulent or not._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pilih data yang hanya bertipe numeric :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = fraud.select_dtypes(\"number\").columns\n",
    "fraud_num = fraud[cols]\n",
    "fraud_num.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melihat nilai covariance pada dataframe `fraud_num` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di atas adalah distribusi nilai covariance dari data yang belum distandarisasi (scale). Variance dari masing-masing variabel berbeda jauh karena range/skala dari tiap variabel berbeda, begitupun covariance. **Nilai variance dan covariance dipengaruhi oleh skala dari data**. Semakin tinggi skala, nilai variance atau covariance akan semakin tinggi.\n",
    "\n",
    "[**Data dengan perbedaan skala antar variabel yang tinggi tidak baik untuk langsung dianalisis PCA karena dapat menimbulkan bias**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing: Scaling\n",
    "\n",
    "Melakukan normalisasi pada dataframe `fraud_num` agar setiap prediktor memiliki scala yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggunakan Z-score standardization untuk scaling dataset numerik dengan fungsi [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) pada library sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "fraud_scaled = scaler.fit_transform(fraud_num.values)\n",
    "\n",
    "fraud_scaled = pd.DataFrame(fraud_scaled, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek covariance setelah di scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Diskusi:</b> kenapa kita menggunakan StandardScaler bukan Min-Max scaling untuk kasus PCA?\n",
    "</div>\n",
    "\n",
    "> jawaban: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_minmax = MinMaxScaler().fit_transform(fraud_num.values)\n",
    "\n",
    "fraud_minmax = pd.DataFrame(fraud_minmax, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 9))\n",
    "plt.subplot(3,1,1)\n",
    "sns.kdeplot(data=fraud.iloc[:,2:7], legend=None)\n",
    "plt.ylabel(\"Base data\")\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "sns.kdeplot(data=fraud_minmax.iloc[:,2:7], legend=None)\n",
    "plt.ylabel(\"MinMaxScaler\")\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "sns.kdeplot(data=fraud_scaled.iloc[:,2:7], legend=None)\n",
    "plt.ylabel(\"StandardScaler\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis menggunakan library [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi objek PCA\n",
    "pca = PCA(n_components = ___, # jumlah pca yang dihasilkan\n",
    "          svd_solver='full') # implementasi full svd sehingga mendapatkan semua PC yang terbentuk\n",
    "\n",
    "pca.fit(____) # menghitung PCA\n",
    "# atau dapat menggunakan pca = pca.fit_transform(scale(balance_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[additional] Note:** jika kita perhatikan bagian dokumentasi pada library scikit-learn, fungsi PCA menggunakan [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) sebagai reduksi dimensi linearnya. Output yang dihasilkan akan tetap sama dengan menggunakan dekomposisi eigen (mencari eigen vector dan eigen value), tetapi komputasi numeriknya lebih stabil dan efisien.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>&#128250; Rekomendasi Video:</b> <a href=\"https://www.youtube.com/watch?v=DQ_BkPHIl-g\" class=\"button large hpbottom\">hubungan PCA dengan SVD </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan banyaknya PC yang terbentuk dengan n_components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pca.components_` : berisi nilai *eigen vector* yang akan dijadikan formula untuk PC baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opsional\n",
    "pd.DataFrame(pca.components_.T, # dibalik/transpose agar representasi tiap pca menjadi kolom, bukan baris\n",
    "             columns=pca.get_feature_names_out()) # ambil nama kolom tiap pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melihat proporsi nilai informasi yang dapat ditangkap untuk setiap PC dengan atribut `explained_variance_ratio_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan banyaknya PC yang terbentuk dengan explained_variance_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melihat kumulatif proporsi nilai informasi yang dapat ditangkap untuk setiap penambahan PC: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "- Proportion of Variance: informasi yang ditangkap oleh tiap PC\n",
    "- Cumulative Proportion: jumlah informasi yang ditangkap secara kumulatif dari PC0 hingga PC tersebut\n",
    "\n",
    "Untuk lebih jelasnya, kita dapat mengeluarkan Cumulative Proportion di atas menggunakan plot di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung proporsi variasi yang dijelaskan oleh setiap komponen utama\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Buat scree plot menggunakan plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot proporsi variasi yang dijelaskan\n",
    "fig.add_trace(go.Scatter(x=list(range(1, len(explained_var_ratio) + 1)), \n",
    "                         y=explained_var_ratio, mode='lines+markers', \n",
    "                         name='Explained Variance Ratio'))\n",
    "\n",
    "# Atur layout dan tampilkan\n",
    "fig.update_layout(title='Scree Plot',\n",
    "                  xaxis_title='Principal Component (PC)',\n",
    "                  yaxis_title='Explained Variance Ratio',\n",
    "                  showlegend=True,\n",
    "                  width=800, height=620)\n",
    "\n",
    "pyo.iplot(fig, 'Scree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform PCA**\n",
    "\n",
    "Menampilkan nilai di setiap PC pada dimensi baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_ = pd.DataFrame(pca.transform(fraud_scaled), \n",
    "                          columns=pca.get_feature_names_out())\n",
    "transform_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Diskusi:</b> ketika kita declare value dari `n_components` sama dengan jumlah dari fitur/variabel datasetnya <b>dan</b> kita menggunakan <b>semua</b> PC yang terbentuk, apakah kita sudah melakukan <b>reduksi dimensi</b>?\n",
    "</div>\n",
    "\n",
    "> jawaban: \n",
    "\n",
    "Reduksi dimensi dengan mempertahankan at least 90% informasi maka PC dipilih sampai ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_pca = ____\n",
    "fraud_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes**: Setelah dipilih PC yang merangkum informasi yang dibutuhkan, PC dapat digabung dengan data awal dan digunakan untuk analisis lebih lanjut (misal: supervised learning).\n",
    "\n",
    "Cara yang dilakukan di atas adalah cara manual, sebenarnya kita bisa secara langsung melakukan reduksi dimensi ketika membuat objek PCA yaitu menuliskan proporsi informasi yang ingin dipertahankan pada parameter `n_components`.\n",
    "\n",
    "Kekurangan dari cara ini adalah kita tidak bisa melakukan detransform ke bentuk awal karena adanya informasi yang hilang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components = ___, # gunakan proporsi data\n",
    "          svd_solver='full')\n",
    "pca2.fit(fraud_scaled.values)\n",
    "\n",
    "fraud_pca90 = pd.DataFrame(pca2.fit_transform(fraud_scaled), \n",
    "                          columns=pca2.get_feature_names_out())\n",
    "\n",
    "fraud_pca90.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[optional] Detransform PCA**\n",
    "\n",
    "Mengembalikan hasil reduksi dimensi menjadi data bentuk aslinya. Tetapi hal ini hanya bisa dilakukan pada data hasil PCA yang masih lengkap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.inverse_transform(transform_)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contoh aplikasi PCA (bahasa pemrograman R):**\n",
    "\n",
    "- sebagai metode untuk mengurangi multikolinearitas: [rpubs](https://rpubs.com/tomytjandra/PCA-reduce-multicollinearity)\n",
    "- sebagai input untuk model klasifikasi: [rpubs](https://rpubs.com/tomytjandra/PCA-before-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita coba bandingkan bagaimana kondisi covariance data kita sebelum discaling, sesudah scaling, dan setelah menjadi bentuk PCA. Silakan jalankan kode berikut ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatif menggunakan seaborn heatmap, sebelum dilakukan scaled\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "sns.heatmap(fraud_num.cov().round(2), vmin=-1, vmax=1, annot=True, cmap='YlGnBu', \n",
    "            annot_kws={\"size\": 5, \"color\":'white', \"alpha\":0.7, \"ha\": 'center', \"va\": 'center'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "sns.heatmap(fraud_scaled.cov().round(2), vmin=-1, vmax=1, annot=True, cmap='YlGnBu',\n",
    "            annot_kws={\"size\": 5, \"color\":'white', \"alpha\":0.7, \"ha\": 'center', \"va\": 'center'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "sns.heatmap(new_data.cov().round(2), vmin=-1, vmax=1, annot=True, cmap='YlGnBu', \n",
    "            annot_kws={\"size\": 5, \"color\":'white', \"alpha\":0.7, \"ha\": 'center', \"va\": 'center'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing PCA\n",
    "\n",
    "PCA tidak hanya berguna untuk dimensionality reduction namun baik untuk visualisasi high-dimensional data. Visualisasi dapat menggunakan **biplot** yang menampilkan:\n",
    "\n",
    "1. **Individual factor map**, yaitu sebaran data secara keseluruhan menggunakan 2 PC. Tujuannya untuk:\n",
    "  - observasi yang serupa\n",
    "  - outlier dari keseluruhan data\n",
    "2. **Variables factor map**, yaitu plot yang menunjukkan korelasi antar variable dan kontribusinya terhadap PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot Visualization\n",
    "\n",
    "Kita akan menggunakan fungsi custom dari helper yaitu `biplot_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method dari helper.py\n",
    "biplot_pca(fraud_scaled[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keterangan:\n",
    "\n",
    "- **Titik/poin observasi:**\n",
    "    + index angka dari observasi.\n",
    "    + Semakin berdekatan maka karakteristiknya semakin mirip, sedangkan yang jauh dari gerombolan data dianggap sebagai outlier\n",
    "    \n",
    "- **Garis vektor:**\n",
    "    + loading score, menunjukkan kontribusi variabel tersebut terhadap PC, atau banyaknya informasi variabel tersebut yang dirangkum oleh PC.\n",
    "    + Semakin jauh panah, semakin banyak informasi yang dirangkum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi biplot (loadings) menggunakan library [plotly](https://plotly.com/python/pca-visualization/#visualize-loadings). Fungsi ini merupakan fungsi custom yang dapat dilihat pada file `helper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot_plotly(fraud_scaled, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual\n",
    "\n",
    "1. **Outlier detection**: observasi yang jauh dari kumpulan observasi lainnya mengindikasikan outlier dari keseluruhan data. Observasi ini dapat ditandai untuk nantinya dicek karakteristik datanya untuk keperluan bisnis, atau apakah mempengaruhi performa model, dll.\n",
    "\n",
    "\n",
    "2. **Observasi searah panah** mengindikasikan observasi tersebut nilainya tinggi pada variabel tersebut. Bila bertolak belakang, maka nilainya rendah pada variable tersebut.\n",
    "\n",
    "\n",
    "3. **Observasi berdekatan**: observasi yang saling berdekatan memiliki karakteristik yang mirip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Variable\n",
    "\n",
    "**Korelasi antar variabel** dapat dilihat dari sudut antar panah: \n",
    "\n",
    "- Panah saling berdekatan (sudut antar panah < 90), maka korelasi positif\n",
    "- Panah saling tegak lurus (sudut antar panah = 90), maka tidak berkorelasi\n",
    "- Panah saling bertolak belakang (sudut antar panah mendekati 180), maka korelasi negatif\n",
    "\n",
    "**Variable Importance**\n",
    "\n",
    "Selain melihat berdasarkan variable factor map, kita juga dapat memetakan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dapatkan loadings dari PCA\n",
    "loadings = pca.components_\n",
    "\n",
    "# Buat dataframe untuk loadings\n",
    "loadings_df = pd.DataFrame(data=loadings.T, \n",
    "                           columns=pca.get_feature_names_out())\n",
    "\n",
    "# Tambahkan kolom nama variabel\n",
    "loadings_df['Variable'] = fraud_scaled.columns\n",
    "\n",
    "# Tampilkan loadings yang signifikan (misalnya, absolute loadings > 0.3)\n",
    "significant_loadings = loadings_df[abs(loadings_df['pca0']) > 0.2]\n",
    "significant_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons PCA\n",
    "\n",
    "Kelebihan melakukan PCA:\n",
    "\n",
    "- Beban komputasi apabila dilakukan pemodelan relatif lebih rendah\n",
    "- Bisa jadi salah satu teknik untuk improve model, namun tidak selalu menjadi lebih baik\n",
    "- Mengurangi resiko terjadinya multikolinearitas, karena nilai antar PC sudah tidak saling berkorelasi\n",
    "\n",
    "Kekurangan melakukan PCA (sebelum pemodelan):\n",
    "\n",
    "- Model tidak dapat diinterpretasikan, karena nilai PC merupakan campuran dari beberapa variabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "## Local Outlier Factor with PyOD\n",
    "\n",
    "**Local Outlier Factor** (LOF) merupakan salah satu algoritma umum yang digunakan untuk kasus anomaly detection. Teknik ini bekerja dengan menghitung skor berdasarkan kepadatan data berdasarkan jaraknya (sangat mirip dengan konsep k-NN). \n",
    "\n",
    "LOF dapat menjadi pilihan yang baik untuk deteksi fraud dalam menentukan anomali data, berikut adalah beberapa kelebihan dan kekurangan dari metode ini.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Efektif dalam menemukan outlier lokal: LOF dapat mengidentifikasi outlier yang tidak dapat ditemukan oleh metode global, seperti outlier yang berada di dalam cluster yang padat.\n",
    "- Tidak sensitif terhadap distribusi data: LOF dapat bekerja dengan baik pada data dengan distribusi yang tidak normal.\n",
    "- Mudah diimplementasikan: LOF dapat diimplementasikan dengan mudah menggunakan library Python seperti Pyod.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Dapat menjadi lambat untuk data yang besar: LOF memerlukan komputasi yang cukup berat untuk dataset yang besar.\n",
    "- Memerlukan pemilihan parameter yang tepat: Parameter k (jumlah tetangga terdekat) yang digunakan dalam LOF dapat mempengaruhi hasil deteksi outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secara sederhana, LOF akan menghitung jarak antar data dan data yang secara kumpulan lokal terisolasi akan didefinisikan sebagai outlier oleh LOF. Berikut adalah ilustrasi sederhana dari kumpulan data dalam ruang 2 dimensi secara lokal.\n",
    "\n",
    "![LOF2](assets/lof2.jpg)\n",
    "\n",
    "Pada ilustrasi di atas, C1 dan C2 merupakan kumpulan data lokal. Titik yang diperhatikan adalah O1, O2, O3, dan O4. \n",
    "\n",
    "Pada kasus kita ini O1 dan O2 dapat dianggap sebagai outlier lokal untuk kelompok C1. Sementara O4 kemungkinan bukan merupakan outlier untuk kelompok C2 karena rentang jarak per data di kelompok C2 cukup renggang/tidak sepadat C1. Sementara O3 dapat dikatakan sebagai outlier global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita akan menggunakan data hasil PCA yaitu `fraud_pca90` untuk mencoba metode ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_pca90.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fungsi `LOF()` dapat digunakan setelah mengakses modul `model.lof` dari library `pyod`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_model = LOF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objek LOF di atas dapat langsung kita gunakan kepada data yang sudah kita olah sebelumnya menggunakan method `fit_predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_label = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena merupakan proses unsupervised, maka metode fit_predict akan langsung menghasilkan label. Tetapi sebenarnya terdapat skor anomali untuk setiap data yang dimasukkan ke model. Skor anomali ini dapat dilihat menggunakan method `decision_function()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung nilai LOF\n",
    "lof_scores = lof_model.decision_function(fraud_pca90)\n",
    "\n",
    "lof_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena merupakan skor setiap data, maka untuk lebih jelasnya kita bisa lihat distribusinya menggunakan histogram ataupun boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sementara untuk label, kita dapat dengan mudah menghitung masing-masing hasil label menggunakan `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lof_label).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter on LOF Model\n",
    "\n",
    "Objek model LOF memiliki beberapa parameter yang dapat kita gunakan, parameter yang paling umum digunakan adalah:\n",
    "\n",
    "- `contamination`: mengatur proporsi estimasi anomali pada data (default = 0.1)\n",
    "- `n_neighbors`: jumlah tetangga yang dianggap sebagai 1 kluster (default = 20)\n",
    "- `metrics`: metode perhitungan jarak yang digunakan\n",
    "\n",
    "<!-- Selain itu kita juga dapat mengatur metode perhitungan jarak yang digunakan dengan parameter `metric`. -->\n",
    "\n",
    "Nilai contamination ini dapat kita isi disesuaikan dengan kasus yang ada, contoh:\n",
    "\n",
    "> Apabila kita ketahui terdapat 1% akun bank BRI merupakan akun yang digunakan untuk penipuan maka kita dapat menggunakan nilai `contamination = 0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_tune = LOF(\n",
    "    contamination = ___,\n",
    "    n_neighbors = ___\n",
    ")\n",
    "\n",
    "lof_label_tune = lof_tune.fit_predict(fraud_pca90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat dampak penggunaan parameter contamination dari jumlah anomali yang dideteksi oleh model kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lof_label_tune).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain melihat plot distribusinya, kita dapat menampilkan persebaran outlier kita pada bidang 2 dimensi hasil PCA. Berikut adalah kodenya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan plot anomali (___ diisi dengan nama dataframe PCA)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=___['pca0'], \n",
    "                y=___['pca1'], \n",
    "                hue=lof_label_tune,\n",
    "                palette='coolwarm')\n",
    "plt.title('Hasil Local Outlier Factor')\n",
    "plt.xlabel(f'PC 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "plt.ylabel(f'PC 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atau untuk lebih jelasnya, kita dapat menggunakan fungsi scatter dari `plotly.express` untuk mengatur posisi legend yang ingin kita lihat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masukkan nama dataframe PCA ke ___\n",
    "___[\"color\"] = lof_label_tune.astype(str)\n",
    "\n",
    "# Plot hasil LOF menggunakan Plotly Express\n",
    "fig = px.scatter(___.sort_values(\"color\"), \n",
    "                 x='pca0', y='pca1', color=\"color\",\n",
    "                 color_discrete_map={'0': '#a6c4ff', '1': '#ffa07a'},\n",
    "                 title='LOF Results',\n",
    "                 labels={'pca0': f'PC 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)',\n",
    "                         'pca1': f'PC 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)'})\n",
    "\n",
    "# Menampilkan plot\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melihat index data yang terdeteksi anomali, kita bisa menggunakan cara berikut ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_indices = np.where(lof_label_tune == 1)[0]\n",
    "anomaly_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga dapat mengambil data yang sifatnya anomali ini menggunakan index yang sudah ditemukan di atas. Dari proses ini kita dapat mentransformasi kembali data kita ke bentuk semula. \n",
    "\n",
    "Ingat bahwa kita sebelumnya membuat dua buah pca yaitu pca yang menyimpan seluruh informasi dan pca yang mengambil 90% informasi. Maka kita gunakan pca yang menyimpan seluruh informasi ini setelah itu kita kembalikan ke bentuk sebelum di scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = fraud_pca.iloc[anomaly_indices]\n",
    "\n",
    "temp = pd.DataFrame(pca.inverse_transform(anomaly))\n",
    "\n",
    "anomaly_df = pd.DataFrame(scaler.inverse_transform(temp), \n",
    "                          columns=fraud_scaled.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
